# ğŸ“Š Retail Insights Assistant

A multi-agent GenAI-powered system for analyzing retail sales data, generating automated business insights, and answering analytical questions in natural language.

## ğŸ¯ Overview

This project implements an intelligent **Retail Insights Assistant** that combines:
- **Multi-Agent Architecture**: Using LangGraph with 4 specialized agents
- **Efficient Data Processing**: DuckDB for high-performance SQL queries
- **LLM Integration**: OpenAI GPT-4 for natural language understanding
- **Interactive UI**: Streamlit-based interface for easy interaction
- **Scalable Design**: Architecture ready for 100GB+ datasets

## ğŸ—ï¸ Architecture

### Multi-Agent System

The system uses **LangGraph** to orchestrate 4 specialized agents:

1. **Query Resolution Agent**
   - Interprets natural language queries
   - Converts user intent to structured queries
   - Generates optimized SQL queries for DuckDB

2. **Data Extraction Agent**
   - Executes SQL queries against DuckDB
   - Retrieves relevant data efficiently
   - Handles both summarization and specific Q&A queries

3. **Validation Agent**
   - Validates extracted data quality
   - Checks for inconsistencies and anomalies
   - Provides confidence scores and recommendations

4. **Response Generation Agent**
   - Generates human-readable insights
   - Formats responses for business users
   - Includes specific metrics and actionable recommendations

### Technology Stack

- **Language**: Python 3.8+
- **LLM Framework**: LangChain, LangGraph, OpenAI GPT-4
- **Data Processing**: DuckDB, Pandas
- **UI**: Streamlit
- **Vector Store** (optional): ChromaDB, FAISS
- **Orchestration**: LangGraph state machine

## ğŸ“‹ Features

### âœ… Implemented Features

- âœ… Multi-agent system with 4 specialized agents
- âœ… Two operational modes:
  - **Summarization Mode**: Comprehensive sales performance summaries
  - **Conversational Q&A Mode**: Ad-hoc analytical questions
- âœ… DuckDB integration for efficient querying
- âœ… Streamlit UI with interactive visualizations
- âœ… Prompt engineering for consistent responses
- âœ… Conversation history and context management
- âœ… Agent workflow visualization
- âœ… Real-time data visualizations (charts and graphs)

### ğŸ¯ Use Cases

1. **Executive Summaries**: Generate comprehensive performance reports
2. **Ad-hoc Analysis**: Answer specific business questions
3. **Trend Analysis**: Identify patterns in sales data
4. **Regional Performance**: Analyze sales by geography
5. **Category Insights**: Understand product category performance

## ğŸš€ Setup Instructions

### Prerequisites

- Python 3.8 or higher
- OpenAI API key
- pip (Python package manager)

### Installation Steps

1. **Clone or extract the project**
   ```bash
   cd blend_assignment
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   ```

   Edit `.env` and add your OpenAI API key:
   ```
   OPENAI_API_KEY=your_openai_api_key_here
   ```

4. **Verify data files**
   Ensure the `Sales Dataset/` folder contains:
   - Amazon Sale Report.csv
   - Sale Report.csv
   - International sale Report.csv

### Running the Application

**Start the Streamlit UI:**
```bash
streamlit run app.py
```

The application will open in your default browser at `http://localhost:8501`

### Using the CLI (Optional)

For programmatic access, you can use the orchestrator directly:

```python
from orchestrator import RetailInsightsOrchestrator
import os

# Initialize
orchestrator = RetailInsightsOrchestrator(
    api_key=os.getenv("OPENAI_API_KEY"),
    data_path="Sales Dataset/"
)

# Generate summary
summary = orchestrator.generate_summary()
print(summary['response'])

# Ask a question
result = orchestrator.process_query(
    "Which category has the highest sales?"
)
print(result['response'])
```

## ğŸ’¡ Usage Examples

### Example Questions (Q&A Mode)

- "Which category saw the highest sales in April 2022?"
- "What is the total revenue by state?"
- "Show me the top 10 performing products"
- "Which region has the most orders?"
- "What is the average order value?"
- "Which customers made the most purchases?"
- "What are the sales trends by month?"

### Summary Mode

Click "Generate Summary" to get a comprehensive overview including:
- Total revenue and order metrics
- Top performing categories
- Regional performance analysis
- Key trends and insights

## ğŸ—‚ï¸ Project Structure

```
blend_assignment/
â”œâ”€â”€ app.py                      # Streamlit UI application
â”œâ”€â”€ orchestrator.py             # LangGraph workflow orchestrator
â”œâ”€â”€ agents.py                   # Multi-agent system implementation
â”œâ”€â”€ data_processor.py           # DuckDB data processing layer
â”œâ”€â”€ config.py                   # Configuration management
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ ARCHITECTURE.md            # 100GB+ scalability architecture
â””â”€â”€ Sales Dataset/             # Sales data CSV files
    â”œâ”€â”€ Amazon Sale Report.csv
    â”œâ”€â”€ Sale Report.csv
    â””â”€â”€ International sale Report.csv
```

## ğŸ”§ Configuration

### Environment Variables

- `OPENAI_API_KEY`: Your OpenAI API key (required)
- `MODEL_NAME`: OpenAI model to use (default: gpt-4-turbo-preview)
- `TEMPERATURE`: LLM temperature (default: 0.1)
- `DATA_PATH`: Path to sales data (default: Sales Dataset/)

### Model Configuration

You can modify the model in `config.py`:
- **GPT-4 Turbo**: Best performance, higher cost
- **GPT-3.5 Turbo**: Faster, lower cost (change in .env)

## ğŸ“Š Data Processing

### DuckDB Integration

- **In-memory database** for fast queries
- **Automatic CSV loading** on initialization
- **Optimized SQL queries** for analytics
- **Supports aggregations** and complex joins

### Supported Datasets

1. **Amazon Sales**: Order-level transaction data
2. **Inventory**: Stock and SKU information
3. **International Sales**: Customer transaction data

## ğŸ§ª Testing

### Manual Testing

1. Start the application
2. Try example questions in Q&A mode
3. Generate summary in Summary mode
4. Verify agent workflow execution
5. Check data visualizations

### Expected Behavior

- Agents should complete successfully (green checkmarks)
- Responses should be contextual and accurate
- Visualizations should display correctly
- Chat history should persist during session

## ğŸš¨ Troubleshooting

### Common Issues

1. **API Key Error**
   - Solution: Ensure `OPENAI_API_KEY` is set in `.env`

2. **Data Loading Error**
   - Solution: Verify CSV files exist in `Sales Dataset/` folder

3. **Module Not Found**
   - Solution: Run `pip install -r requirements.txt`

4. **Slow Performance**
   - Solution: Consider using GPT-3.5-turbo for faster responses

## ğŸ¨ UI Features

- **Dual Mode Interface**: Switch between Q&A and Summary modes
- **Agent Workflow Visualization**: See each agent's status
- **Interactive Charts**: Plotly-based visualizations
- **Chat History**: Persistent conversation history
- **Responsive Design**: Works on different screen sizes

## ğŸ” Security & Privacy

- API keys stored in environment variables (not in code)
- No data sent to external services except OpenAI API
- Local data processing with DuckDB
- No persistent storage of conversations

## ğŸ“ˆ Performance Considerations

- **Current Scale**: Optimized for datasets up to 10GB
- **In-memory Processing**: Fast queries with DuckDB
- **LLM Caching**: Reduces API calls for similar queries
- **Async Processing**: Non-blocking UI operations

For 100GB+ scaling strategy, see **ARCHITECTURE.md**

## ğŸ”® Future Enhancements

### Potential Improvements

1. **Vector Embeddings**: Add FAISS for semantic search
2. **Query Caching**: Cache common query results
3. **Batch Processing**: Support bulk data analysis
4. **Export Features**: Download reports as PDF/Excel
5. **Authentication**: Add user management
6. **Real-time Data**: Streaming data ingestion
7. **Multi-language**: Support for non-English queries

## ğŸ¤ Assumptions & Limitations

### Assumptions

- CSV files are properly formatted
- Dates are in consistent format
- Currency is in INR (Indian Rupees)
- Data quality is reasonable (minimal nulls/errors)

### Current Limitations

1. **API Dependency**: Requires OpenAI API access
2. **Memory Constraints**: In-memory DuckDB limited by RAM
3. **No Persistence**: Chat history not saved between sessions
4. **Limited Time-series**: No advanced forecasting
5. **Single User**: Not designed for concurrent users

## ğŸ“š Technical Notes

### Agent Communication

Agents communicate through a shared **AgentState** object:
```python
class AgentState(TypedDict):
    user_query: str
    query_type: str
    structured_query: Optional[str]
    sql_query: Optional[str]
    extracted_data: Optional[Dict]
    validation_result: Optional[Dict]
    final_response: Optional[str]
    errors: List[str]
    metadata: Dict
```

### LangGraph Workflow

The workflow follows a linear pipeline:
```
Query Resolution â†’ Data Extraction â†’ Validation â†’ Response Generation
```

Each agent can:
- Read the current state
- Modify relevant fields
- Add errors or metadata
- Pass control to the next agent

### Prompt Engineering

Key prompt strategies used:
- **System prompts** for consistent behavior
- **Few-shot examples** (implicit in prompts)
- **Structured output** (JSON formatting)
- **Context preservation** across agents
- **Temperature control** for consistency

## ğŸ“„ License

This project is created for the Blend360 GenAI Interview Assignment.

## ğŸ‘¥ Contact

For questions or issues, please refer to the assignment submission guidelines.

---

**Built with** â¤ï¸ **using LangGraph, OpenAI GPT-4, DuckDB, and Streamlit**
